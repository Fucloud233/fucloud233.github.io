---
layout: post
title: 特征向量与特征值的理解
categories: math
---

# 背景

# 他们怎么来的

特征值向量和特征值这两玩意儿，见到的也不是一次两次了
从线性代数，到概率统计，最后到我现在正在学习的计算机图形学，都不乏它的身影。
可见，其在不管是的数学还是计算机领域中，都有举足轻重的作用。
可是，每当提起到，都难免有些陌生感。

特征值的特征向量最常见的表示形式：
其$A$为变换矩阵，$\lambda$为特征值，$\overrightarrow{v}$为特征向量。
$$
A\overrightarrow{v} = \lambda\overrightarrow{v}
$$

这个式子一开始我也没搞动什么含义，但通过数形结合的方式，还是能够窥见其二。

在原始坐标系中，坐标轴的基为`(0, 1)`和`(1, 0)`。 
现在我们想进行坐标变换，让他变为以(2, 1)和(2, 3)为基的坐标轴。
在数学上的定义方式就是，如上面的公式所示，
$$
A = \begin{bmatrix}
2 & 2 \\
1 & 3
\end{bmatrix},\quad
\overrightarrow{v} = \begin{bmatrix}
x\\ y
\end{bmatrix},\quad

\overrightarrow{v}' = A\cdot \overrightarrow{v}\\
$$

> 以上的式子也变相地解释了变换矩阵是如何应用在坐标变换的过程中的。
> 我们可以将上述的式子展开可得
> $$
> x' = 2x + 2y,\quad y' = x + 3y
> $$
> 其实我也不知道

---

对于从原点出发的任意一个向量，你会发现，总有一些向量的方向总会保持不变，也就是图中的v1。
这些向量我们就称为特征向量。
而变换后的向量与原向量的比值，我们就称为特征值。

# 如何求解出特征值和特征向量

求解特征值和特征向量的方式其实还是要回答上面的常见形式。
通过移项，我们可以得到
$$
A\overrightarrow{v} = \lambda\overrightarrow{v}
\to (A - \lambda)\overrightarrow{v}=0 
\to \det(A - \lambda) = 0
$$
> 特征矩阵不可能为0，所以若等式为0，则说明$A-\lambda$线性相关，也就是其行列式为0。

以上就是如何求解出特征值的思路了，求解过程在这里就不再赘述了。

对于一个矩阵，一般来说它拥有的特征向量是和维度相同的，当然也有可能出现没有特征值和特征向量的情况。

---

以上我们能够求解出特征值，而求解特征向量，则需要使用一下方程进行求解。在求解的过程可能会发现已知条件不够，也就是说不能准确求出未知数的值。
但其实特征向量只是描述方向，具体长度其实并不重要，
因此我们令其中一个未知数为任意常数即可。
$$
(A - \lambda)\overrightarrow{v} = 0,\quad \overrightarrow{v} = \begin{bmatrix} x\\ y\end{bmatrix}
$$

# 如何理解特征向量

特征向量相比于其他向量而言，最突出的地方正如前面所言，是在坐标变换时能够保持不变的向量。

而对于特征值，他既是表示特征向量变换之后的递增倍数，对应以下等式。
这个等式也说明，对于特征向量而言，通过矩阵变换，并不会改变向量的方向，
而是改变向量，而倍数恰好是特征值。

$$
A\overrightarrow{v} = \lambda\overrightarrow{v}
$$

同时，另一点可以说明，可以通过特征值来对向量进行降维。
$$
\det(A - \lambda) = 0
$$




